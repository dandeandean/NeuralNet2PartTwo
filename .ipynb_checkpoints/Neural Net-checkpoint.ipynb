{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d2e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad2d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used the same Inputs, function, and weights from the NN Example for this one:\n",
    "#Activations [[I1, I2], [H1, H2], [O1]]:\n",
    "TARGET_ACTIVATIONS = [[1.0, 1.0], [0.5602064588081933, 0.5989278971297447], [0.45564373499655964]]\n",
    "\n",
    "#Weights [[W_I1H1, W_I1H2, W_I2H1, W_I2H2], [W_H1O1, W_H2O1]]\n",
    "TARGET_WEIGHTS = [[-0.071, 0.078, 0.313, 0.323], [-0.34, 0.021]]\n",
    "\n",
    "#The errors (error signal) input into calculate_deltas (basically just the 1st little delta):\n",
    "TARGET_ERROR_SIG = [[0, 0], [0, 0], [-0.1350180571419062]]\n",
    "\n",
    "#The output from calculate_deltas (tuple with 3 2d-lists):\n",
    "TARGET_CALC_DELTAS = ([[0, 0], [0.011310133471139936, -0.0006810957126331252], [-0.1350180571419062]],\n",
    "[[0.0022620266942279875, -0.00013621914252662506, 0.0022620266942279875, -0.00013621914252662506], [-0.015127597533325916, -0.01617321620770912]],\n",
    "[[0, 0], [0.0022620266942279875, -0.00013621914252662506], [-0.027003611428381244]])\n",
    "\n",
    "#The output from adjust_weights_bias (tuple with 2 2d-lists):\n",
    "TARGET_ADJUST_WEIGTHS_BIAS=([[-0.07326202669422797, 0.07813621914252662, 0.310737973305772, 0.32313621914252666], [-0.3248724024666741, 0.03717321620770912]],\n",
    "[[0, 0], [-0.0022620266942279875, 0.00013621914252662506], [0.027003611428381244]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fd04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200384ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mN\u001b[49m\u001b[38;5;241m.\u001b[39mmatrixify_weigths(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#The key to the feed forward algorithm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sigmoid(numpy\u001b[38;5;241m.\u001b[39mdot(numpy\u001b[38;5;241m.\u001b[39mmatrix(c),inputs))\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "c = N.matrixify_weigths(0)\n",
    "#The key to the feed forward algorithm\n",
    "sigmoid(numpy.dot(numpy.matrix(c),inputs)).tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMMAgent(object):\n",
    "    '''\n",
    "    Class to for Neural Net Agents that compete in the Mob task\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_in_nodes, num_hid_nodes, num_hid_layers, num_out_nodes, \\\n",
    "                learning_rate = 0.2, max_epoch=10000, min_sse=.01, momentum=0, \\\n",
    "                creation_function=None, activation_function=None, random_seed=1):\n",
    "        '''\n",
    "        Arguments:\n",
    "            num_in_nodes -- total # of input nodes for Neural Net\n",
    "            num_hid_nodes -- total # of hidden nodes for each hidden layer\n",
    "                in the Neural Net\n",
    "            num_hid_layers -- total # of hidden layers for Neural Net\n",
    "            num_out_nodes -- total # of output layers for Neural Net\n",
    "            learning_rate -- learning rate to be used when propogating error\n",
    "            max_epoch -- maximum number of epochs for our NN to run during learning\n",
    "            min_sse -- minimum SSE that we will use as a stopping point\n",
    "            momentum -- Momentum term used for learning\n",
    "            creation_function -- function that will be used to create the\n",
    "                neural network given the input\n",
    "            activation_function -- list of two functions:\n",
    "                1st function will be used by network to determine activation given a weighted summed input\n",
    "                2nd function will be the derivative of the 1st function\n",
    "            random_seed -- used to seed object random attribute.\n",
    "                This ensures that we can reproduce results if wanted\n",
    "        '''\n",
    "        assert num_in_nodes > 0 and num_hid_layers > 0 and num_hid_nodes and num_out_nodes > 0, \"Illegal number of input, hidden, or output layers!\"\n",
    "        self.num_in_nodes = num_in_nodes\n",
    "        self.num_hid_nodes = num_hid_nodes\n",
    "        self.num_hid_layers = num_hid_layers\n",
    "        self.num_out_nodes = num_out_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epoch = max_epoch\n",
    "        self.min_sse = min_sse\n",
    "        self.momentum = momentum\n",
    "        self.creation_function = creation_function\n",
    "        self.activation_function = activation_function\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        #WE CAN SIMPLY FILL OUT THESE VALUES LATER AS WE CALCULATE THEM, BUT WE INIT HERE\n",
    "        (self.weights,self.weight_ds,self.activations,self.errors,self.bias,self.bias_ds) = \\\n",
    "        NeuralMMAgent.create_neural_structure(num_in_nodes, num_hid_nodes,num_hid_layers, num_out_nodes, random.Random(self.random_seed))\n",
    "        \n",
    "    ##################################################################################################\n",
    "    #DONE FUNCTIONS\n",
    "    ##################################################################################################\n",
    "            \n",
    "    def _get_error_sig(self,activations,expected_output): \n",
    "        \"\"\"INPUTS: 2D activations array, 1D expected output array\n",
    "            OUTPUTS: the 2D error signal array\n",
    "        \"\"\"\n",
    "        out = activations.copy()\n",
    "        print(activations[-1],expected_output)\n",
    "        ds = self.sigmoid_af_deriv\n",
    "        err = [ -(expected_output[i] - activations[-1][i]) for i in range(len(expected_output))]\n",
    "        err = [ds(activations[-1][i])*err[i] for i in range(len(err))]\n",
    "        for i in range(len(out)):\n",
    "            for j in range(len(out[i])):\n",
    "                out[i][j] = 0\n",
    "        out[-1] = err\n",
    "        return out\n",
    "\n",
    "    def _calc_all_errors(self, input_list, output_list, activations):\n",
    "        \"\"\"Inputs: input_list, output_list, 2D array activations\n",
    "            Outputs: 2D array of errors at all the activations \n",
    "            Calculates all of the errors in the network\n",
    "            This is the first step to calculating d_errors\n",
    "            \"\"\"\n",
    "        errors = activations.copy()\n",
    "        output_layer = list()\n",
    "        \n",
    "        #OUTPUT LAYER\n",
    "        last_layer = activations[-1]\n",
    "        assert len(last_layer) == len(output_list)\n",
    "        errors[-1] = self.__sub_arrays(output_list,last_layer)\n",
    "        \n",
    "        #HIDDEN LAYERS\n",
    "        for i in range(len(activations)-2,-1,-1):\n",
    "            current_layer = errors[i].copy()\n",
    "            already_calculated_layer = errors[i+1]\n",
    "            for j in range(len(current_layer)):\n",
    "                        #MULTIPLY EACH OUTGOING WEIGHT WITH THE NODE IT MAPS TO IN THE NEXT LAYER\n",
    "                for wi,node in zip(self._get_outgoing_weights(i,j),already_calculated_layer):\n",
    "                    current_layer[j] = wi*node\n",
    "            errors[i] = current_layer\n",
    "        return errors\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        #########ACCESSORS\n",
    "\n",
    "    def get_weights(self):\n",
    "        return (self.weights)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def get_bias(self):\n",
    "        return (self.bias)\n",
    "\n",
    "    def set_biases(self, bias):\n",
    "        self.bias = bias\n",
    "    \n",
    "    def set_thetas(self, thetas):\n",
    "        self.thetas = thetas\n",
    "    \n",
    "    def __add_arrays(self,a,b): return [ a[i] + b[i] for i in range(len(a))]\n",
    "    \n",
    "    def __sub_arrays(self,a,b): return [ a[i] - b[i] for i in range(len(a))]\n",
    "    \n",
    "    def __mult_arrays(self,a,b): return [ a[i] * b[i] for i in range(len(a))]\n",
    "\n",
    "    def _feed_forward(self, input_list, row):\n",
    "        '''Used to feedforward input and calculate all activation values\n",
    "            Arguments:\n",
    "                input_list -- a list of possible input values\n",
    "                row -- the row from that input_list that we should use\n",
    "            Outputs:\n",
    "                list of activation values'''\n",
    "        #DEAL WITH THE INPUT LAYER\n",
    "        activations = [input_list[row]]\n",
    "        assert len(self.weights)+1 == len(self.bias), \"There must be biases for the input layer\"\n",
    "        \n",
    "        \n",
    "        #DEAL WITH THE HIDDEN LAYERS\n",
    "        for i in range(self.num_hid_layers):\n",
    "            old_layer = activations[i]\n",
    "            # make new layer by passing current_layer and weights\n",
    "            new_layer = self._internal_pass(old_layer, self.weights[i] )\n",
    "            new_layer = self.__add_arrays(new_layer,self.bias[i+1]) #i+1 because we add biases for input for some reason\n",
    "            new_layer = list(map(self.sigmoid_af,new_layer))\n",
    "            activations.append(new_layer)\n",
    "            \n",
    "            \n",
    "        #DEAL WITH THE OUTPUT LAYER\n",
    "        out = list()\n",
    "        for i in range(self.num_out_nodes):\n",
    "            incoming_weights = self._get_incoming_weights(self.num_hid_layers+1,i)\n",
    "            incoming_activations = activations[-1]\n",
    "            new_act = sum(self.__mult_arrays(incoming_weights,incoming_activations))\n",
    "            new_act = self.sigmoid_af(new_act)\n",
    "            out.append(new_act)\n",
    "        activations.append(out)\n",
    "        return activations \n",
    "\n",
    "    def _internal_pass(self,prev_layer,weights): \n",
    "        \"\"\"Takes the activations of the previous layer \n",
    "            & computes the activation weights of the \n",
    "            new weights without bias or sigmoid\"\"\"\n",
    "        assert len(prev_layer)**2 == len(weights), \"Internal Pass failed, vectors not compatible\"\n",
    "        depth = len(prev_layer)\n",
    "        offset = 0\n",
    "        new_layer_activations_raw = [0]*len(prev_layer)\n",
    "        while offset < depth:\n",
    "            for i in range(offset,len(weights),depth):\n",
    "                new_layer_activations_raw[offset] += weights[i]*prev_layer[i//depth] #i//depth to cycle through inputs\n",
    "            offset += 1\n",
    "        return new_layer_activations_raw\n",
    "    \n",
    "        \n",
    "    def _get_incoming_weights(self,layer_index,node_index):\n",
    "        incoming_weights = list()\n",
    "        if not layer_index: return [] #zeroth layer is input should not have incoming weights\n",
    "                          # is it a hidden node?                                or is it an output node?\n",
    "        depth = self.num_hid_nodes*(layer_index <= self.num_hid_layers) + self.num_out_nodes*(layer_index > self.num_hid_layers)\n",
    "        for i in range(node_index,len(self.weights[layer_index-1]),depth):\n",
    "            incoming_weights.append(self.weights[layer_index-1][i])\n",
    "        return incoming_weights\n",
    "\n",
    "    \n",
    "    def _get_outgoing_weights(self,layer_index,node_index):\n",
    "        if layer_index == self.num_hid_layers+1: return []\n",
    "        assert(len(self.weights[layer_index])>(node_index))\n",
    "        \n",
    "        outgoing_weights = list()\n",
    "        depth = self.num_hid_nodes*(layer_index <= self.num_hid_layers) + self.num_out_nodes*(layer_index > self.num_hid_layers)\n",
    "        if layer_index == self.num_hid_layers+1: return []\n",
    "        #a straightforward way of getting the # of nodes in the next layer \n",
    "        nodes_in_next_layer = self.num_hid_nodes*(layer_index+1 <= self.num_hid_layers) + self.num_out_nodes*(layer_index+1 > self.num_hid_layers)\n",
    "        for i in range(0,nodes_in_next_layer):\n",
    "            outgoing_weights.append(self.weights[layer_index][node_index*nodes_in_next_layer +i])\n",
    "        return outgoing_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_af(summed_input):\n",
    "        #Sigmoid function\n",
    "        e_to_the = numpy.exp #pythonic\n",
    "        denom = 1+e_to_the(-summed_input)\n",
    "        return 1/denom \n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_af_deriv(sig_output):\n",
    "        #the derivative of the sigmoid function\"\n",
    "        return sig_output * (1-sig_output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _0matrix(row,col):\n",
    "        new_array = []\n",
    "        col_v = [0]*col\n",
    "        for i in range(row):\n",
    "            new_array.append(col_v)\n",
    "        return new_array\n",
    "\n",
    "    @staticmethod\n",
    "    def create_neural_structure(num_in, num_hid, num_hid_layers, num_out, rand_obj):\n",
    "        #2D array with initial weights \n",
    "        weights = NeuralMMAgent._0matrix(num_hid_layers,num_hid**2)\n",
    "        weights.append([0]*num_out*num_hid)\n",
    "        for i in range(len(weights)):\n",
    "            for j in range(len(weights[i])):\n",
    "                weights[i][j] = rand_obj.uniform(-0.5,0.5)\n",
    "        #2D list/array for weight deltas\n",
    "        weights_ds = NeuralMMAgent._0matrix(num_hid_layers,num_hid**2)\n",
    "        weights_ds.append([0]*num_out*num_hid)\n",
    "        #2D list/array for activations\n",
    "        acts = NeuralMMAgent._0matrix(num_hid_layers,num_hid)\n",
    "        acts.insert(0,[0]*num_in)\n",
    "        acts.append([0]*num_out)\n",
    "        #2D list/array for errors\n",
    "        #2D list/array of biases\n",
    "        #2D list/array for biases deltas\n",
    "        \n",
    "        return (weights, [weights_ds],\n",
    "                [acts], [acts],\n",
    "                [acts], [acts])\n",
    "    \n",
    "    ##################################################################################################\n",
    "    #FUNCTIONS IN PROGRESS\n",
    "    ##################################################################################################\n",
    "\n",
    "    def _adjust_weights_bias(self, weight_deltas, bias_deltas):\n",
    "        '''Used to apply deltas\n",
    "        Outputs:\n",
    "            A tuple w/ the following items (in order):\n",
    "            2d list of all weights after updating (e.g. [[-0.071, 0.078, 0.313, 0.323], [-0.34, 0.021]])\n",
    "            list of all biases after updating (e.g., [[0, 0], [0, 0], [0]])\n",
    "        '''\n",
    "        weights = self.weights.copy()\n",
    "        bias = self.bias.copy()\n",
    "        assert (len(self.weights) == len(weight_deltas)) & (len(self.bias == bias_deltas))\n",
    "        for i in self.weigths:\n",
    "            weights[i] = self.__add_arrays(self.weights[i],weight_deltas[i])\n",
    "        for j in self.bias:\n",
    "            bias[j] = self.__add_arrays(self.bias[j],bias_deltas[j])\n",
    "        return (bias,weights)\n",
    "    \n",
    "\n",
    "        \n",
    "    def train_net(self, test_in, test_out,min_sse,max_num_epoch):\n",
    "        # TODO\n",
    "        return\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "    def train_net_incremental(self, input_list, output_list, max_num_epoch=100000, \\\n",
    "                    min_sse=0.001):\n",
    "        ''' Trains neural net using incremental learning\n",
    "            (update once per input-output pair)\n",
    "            Arguments:\n",
    "                input_list -- 2D list of inputs\n",
    "                output_list -- 2D list of outputs matching inputs\n",
    "            Outputs:\n",
    "                1d list of errors (total error each epoch) (e.g., [0.1])\n",
    "        '''\n",
    "        error_vector = list()\n",
    "        total_error_from_epoch = list()\n",
    "        for i in range(len(input_list)):\n",
    "            for e in range(max_num_epoch):\n",
    "                output = self._feed_forward(input_list,i)\n",
    "                error_vector += self.__sub_arrays(output_list[i], output[-1])\n",
    "                tot_err = sum(error_vector)\n",
    "                #(lil_ds,weight_ds,bias_ds) = self._calculate_deltas(output, activations, errors, prev_weight_deltas)\n",
    "                #self._adjust_weights_bias(output, biases)\n",
    "            total_error_from_epoch.append(tot_err)\n",
    "            error_vector = []\n",
    "        return total_error_from_epoch\n",
    "\n",
    "    \n",
    "\n",
    "    def _calculate_deltas(self, activations, errors, prev_weight_deltas=None):\n",
    "        '''Used to calculate all weight deltas for our neural net\n",
    "            Parameters:\n",
    "                activations -- a 2d list of activation values\n",
    "                errors -- a 2d list of errors\n",
    "                prev_weight_deltas [OPTIONAL] -- a 2d list of previous weight deltas\n",
    "            Output:\n",
    "                A tuple made up of 3 items:\n",
    "                    A 2d list of little deltas (e.g., [[0, 0], [-0.1, 0.1], [0.1]])\n",
    "                    A 2d list of weight deltas (e.g., [[-0.1, 0.1, -0.1, 0.1], [0.1, 0.1]])\n",
    "                    A 2d list of bias deltas (e.g., [[0, 0], [-0.1, 0.1], [0]])\n",
    "        '''\n",
    "        little_deltas = activations.copy() # to get the correct dimensions \n",
    "        weight_deltas = self.weights.copy()\n",
    "        bias_deltas = self.activations.copy()\n",
    "        #TO CALCULATE LITTLE DELTAS:\n",
    "        #we need to take the activations * sigmoid_af_deriv\n",
    "        dacts = activations.copy()\n",
    "        for i in range(len(dacts)):\n",
    "            for j in range(len(dacts[i])):\n",
    "                dacts[i][j] = self.sigmoid_af_deriv(dacts[i][j])\n",
    "                \n",
    "                \n",
    "    def matrixify_weigths(self,layer):\n",
    "        layer_in_question = weights[layer]\n",
    "        cols = list()\n",
    "        for i in range(len(layer_in_question)//self.num_hid_nodes):\n",
    "            cols.append(self._get_incoming_weights(layer+1,i))\n",
    "        return cols\n",
    "    \n",
    "    def feedforwardCHAD(self,inp):\n",
    "        #simply shoe the input layer in there\n",
    "        activations = [ [float(i) for i in inp] ]\n",
    "        # with biases\n",
    "        for i in range(len(activations[0])): activations[0][i] += self.bias[0][i]\n",
    "        for i in range(self.num_hid_layers+1):\n",
    "            # change the order of the weights so we can take the dot product with the previous layer of activations \n",
    "            activations.append(sigmoid(numpy.dot(numpy.matrix(self.matrixify_weigths(i)),activations[-1])+self.bias[i+1]).tolist()[0])\n",
    "        return activations\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f08c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[-0.071, 0.078, 0.313, 0.323], [-0.34, 0.021]]\n",
    "inputs = [1,1]\n",
    "biass = [[0,0],[0,0],[0]]\n",
    "outputs = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8700b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weights(weights)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_biases(biass)\n\u001b[0;32m----> 5\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[43mN\u001b[49m\u001b[38;5;241m.\u001b[39mfeedforwardCHAD([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "self = NeuralMMAgent(2,2,1,1)\n",
    "self.set_weights(weights)\n",
    "self.set_biases(biass)\n",
    "\n",
    "activations = N.feedforwardCHAD([1,1]) #== [[1.0, 1.0], [0.5602064588081933, 0.5989278971297447], [0.45564373499655964]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31630cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa41a3e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#this is the error signal\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m little_d \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mmatrix(outputs) \u001b[38;5;241m-\u001b[39m numpy\u001b[38;5;241m.\u001b[39mmatrix(\u001b[43mactivations\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# instead of doing the \"cost derivative,\" like in the text, we will just multipy by -1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m little_d \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mNeuralMMAgent\u001b[38;5;241m.\u001b[39msigmoid_af_deriv(little_d) \u001b[38;5;241m*\u001b[39mlittle_d)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activations' is not defined"
     ]
    }
   ],
   "source": [
    "#this is the error signal\n",
    "little_d = numpy.matrix(outputs) - numpy.matrix(activations[-1])\n",
    "# instead of doing the \"cost derivative,\" like in the text, we will just multipy by -1\n",
    "little_d = (-NeuralMMAgent.sigmoid_af_deriv(little_d) *little_d).tolist()\n",
    "#the rest can just can just be zero for now\n",
    "error_sig = NeuralMMAgent._0matrix(1,self.num_in_nodes) +NeuralMMAgent._0matrix(self.num_hid_layers,N.num_hid_nodes) + little_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464ca370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error_sig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#backprop\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m little_ds \u001b[38;5;241m=\u001b[39m \u001b[43merror_sig\u001b[49m\n\u001b[1;32m      3\u001b[0m previous_layer \u001b[38;5;241m=\u001b[39m error_sig[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hid_layers):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# we will read the list backwards, but we need an offset for the intput and output layers\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'error_sig' is not defined"
     ]
    }
   ],
   "source": [
    "#backprop\n",
    "little_ds = error_sig\n",
    "previous_layer = error_sig[-1]\n",
    "for layer in range(self.num_hid_layers):\n",
    "    # we will read the list backwards, but we need an offset for the intput and output layers\n",
    "    negative_layer_with_offset = -(layer +2)\n",
    "    little_ds[negative_layer_with_offset] = numpy.dot(previous_layer, \n",
    "                                                      numpy.matrix(activations[negative_layer_with_offset]))\n",
    "    \n",
    "    #little_ds[negative_layer_with_offset] *= self.sigmoid_af_deriv(numpy.matrix(previous_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14fcba35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'little_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlittle_ds\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'little_ds' is not defined"
     ]
    }
   ],
   "source": [
    "little_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ba4a5b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m b \u001b[38;5;241m=\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmatrix(\u001b[43mactivations\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'activations' is not defined"
     ]
    }
   ],
   "source": [
    "b =numpy.matrix(activations[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef8c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.matrix([-0.1350180571419062])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac101c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m numpy\u001b[38;5;241m.\u001b[39mdot(\u001b[43mb\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(),a) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_af_deriv(a)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "numpy.dot(b.transpose(),a) * self.sigmoid_af_deriv(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2cb5848",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (a\u001b[38;5;241m*\u001b[39m\u001b[43mb\u001b[49m)\u001b[38;5;241m.\u001b[39mtranspose() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_af_deriv(a)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "(a*b).transpose() * self.sigmoid_af_deriv(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad41935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0841c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60c698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
